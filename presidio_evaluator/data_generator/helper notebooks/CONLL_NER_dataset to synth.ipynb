{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the CONLL2003 dataset using deepavlov, and creates templates (utterances with placeholders) for a PII synthetic data generator to use in order to create new sentences.\n",
    "\n",
    "The notebook additionally introduces two new entities: TITLE and ROLE, in order to overcome cases like \"UK David Scott called his wife\", where the original sentence is \"UK Prime Minister Boris Johnson called his wife\" as \"Prime Minister\" was originally tagged as PER in the original dataset. Same logic goes for titles, like Mr., Mrs., Ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from deeppavlov.dataset_readers.conll2003_reader import Conll2003DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "reader = Conll2003DatasetReader()\n",
    "dataset = reader.read(data_path =\"../../data\",dataset_name='conll2003')\n",
    "#Note: make sure you haven't downloaded something else with this function before, \n",
    "# as it will not download a new dataset (even if your previous download was for a different dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To pandas + add sentence_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = [list(zip(a,b)) for a,b in dataset['train']]\n",
    "df_list = []\n",
    "sentence_id = 0\n",
    "for sentence in new_dataset:\n",
    "   \n",
    "    df = pd.DataFrame(sentence,columns = [\"word\",\"tag\"])\n",
    "    df[\"sentence_idx\"] = sentence_id\n",
    "    sentence_id+=1\n",
    "    df_list.append(df)\n",
    "ner_dataset = pd.concat(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ner_dataset.groupby('sentence_idx')['word'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique entities\n",
    "ner_dataset['tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace tokenization replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['word'] = ner_dataset['word']\\\n",
    ".replace('-LRB-','(')\\\n",
    ".replace('-RRB-',')')\\\n",
    ".replace('-LCB-','(')\\\n",
    ".replace('-RCB-',')')\\\n",
    ".replace('``','\"')\\\n",
    ".replace(\"''\",'\"')\\\n",
    ".replace('/.','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper columns:\n",
    "ner_dataset['prev-word'] = ner_dataset.word.shift(1)\n",
    "ner_dataset['prev-prev-word'] = ner_dataset['word'].shift(2)\n",
    "ner_dataset['next-word'] = ner_dataset['word'].shift(-1)\n",
    "ner_dataset['next-next-word'] = ner_dataset['word'].shift(-2)\n",
    "ner_dataset['prev-tag'] = ner_dataset['tag'].shift(1)\n",
    "ner_dataset['next-tag'] = ner_dataset['tag'].shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unneeded (non PII) entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS_TO_IGNORE = ['CARDINAL','FAC','LAW','LANGUAGE','MISC','TIME','DATE','ORDINAL','EVENT','QUANTITY','WORK_OF_ART','MONEY','PRODUCT','PERCENT']\n",
    "def remote_unwanted_tags(x):\n",
    "    if len(x)>1 and x[2:] in TAGS_TO_IGNORE:\n",
    "        return 'O'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "ner_dataset['tag'] = ner_dataset['tag'].apply(remote_unwanted_tags)\n",
    "ner_dataset[ner_dataset['sentence_idx']==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove PERSON tags if preceding word is 'the' (e.g. the Bush administration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing PERSON tags from sentences with a 'the' preceding the person:\n",
    "\n",
    "def remove_tag_if_the_person(row):\n",
    "    if row['prev-word'].lower() == 'the' and row['tag']=='B-PERSON':\n",
    "        return 'O'\n",
    "    elif row['prev-prev-word'].lower() == 'the' and row['prev-tag']=='I-PERSON' and row['tag']=='B-PERSON':\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "\n",
    "ner_dataset['prev-word']=ner_dataset['prev-word'].astype('str')\n",
    "ner_dataset['prev-prev-word']=ner_dataset['prev-prev-word'].astype('str')\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_person,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove tag from 's (Joe Wilson's cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_if_apostraphe_after_tag(row):\n",
    "    if row['prev-tag'] != 'O' and row['word']==\"'s\":\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_person,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-tag words from dictionaries (countries, nationalities, roles, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nationalities and countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalities = pd.read_csv(\"../raw_data/nationalities.csv\")\n",
    "nationalities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"algeria\" in nationalities['country'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ner_dataset['metadata'] = None\n",
    "\n",
    "def get_nationality_as_metadata(row):\n",
    "    if row['word'].lower() in nationalities['country'].values:\n",
    "        return 'COUNTRY'\n",
    "    elif row['word'].lower() in nationalities['nationality'].values:\n",
    "        return 'NATIONALITY'\n",
    "    elif row['word'].lower() in nationalities['man'].values:\n",
    "        return 'NATION_MAN'\n",
    "    elif row['word'].lower() in nationalities['woman'].values:\n",
    "        return 'NATION_WOMAN'\n",
    "    elif row['word'].lower() in nationalities['plural'].values:\n",
    "        return 'NATION_PLURAL'\n",
    "    return row['metadata']\n",
    "\n",
    "row = pd.Series({'word':'Frenchwoman','metadata':None})\n",
    "print(\"Example: Frenchwoman -> \",get_nationality_as_metadata(row))\n",
    "\n",
    "def update_tag_based_on_metadata(row):\n",
    "    if row['metadata'] is not None:\n",
    "        return \"B-\"+row['metadata']\n",
    "    else:\n",
    "        return row['tag']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['metadata'] = ner_dataset.apply(get_nationality_as_metadata, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_TITLES = ['mr', 'dr', 'professor', 'eng','prof','doctor']\n",
    "FEMALE_TITLES = ['mrs', 'ms', 'miss', 'dr', 'professor', 'eng', 'prof','doctor']\n",
    "\n",
    "def get_title_as_metadata(row):\n",
    "    if row['word'].lower() in MALE_TITLES:\n",
    "        return 'MALE_TITLE'\n",
    "    elif row['word'].lower() in FEMALE_TITLES:\n",
    "        return 'FEMALE_TITLE'\n",
    "    return row['metadata']\n",
    "\n",
    "\n",
    "def update_title_tag_if_missing(row):\n",
    "    if row['word'].lower() in MALE_TITLES and row['tag']=='O':\n",
    "        return 'B-MALE_TITLE'\n",
    "    elif row['word'].lower() in FEMALE_TITLES and row['tag']=='O':\n",
    "        return 'B-FEMALE_TITLE'\n",
    "    else:\n",
    "        return row['tag']\n",
    "\n",
    "ner_dataset['metadata'] = ner_dataset.apply(get_title_as_metadata,axis=1)\n",
    "ner_dataset['tag'] = ner_dataset.apply(update_title_tag_if_missing,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 'the' from 'the NORP' if NORP is not in nationalities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_if_the_norp(row):\n",
    "    if row['prev-word'].lower() == 'the' and row['tag']=='B-NORP' and row['metadata'] is None:\n",
    "        return 'O'\n",
    "    elif row['prev-prev-word'].lower() == 'the' and row['prev-tag']=='I-NORP' and row['tag']=='B-NORP' and row['metadata'] is None:\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_norp,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sentences with adjacent different entities (e.g calling from New York Larry King)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['entity'] = ner_dataset['tag'].str[2:]\n",
    "ner_dataset['next-entity']=ner_dataset['next-tag'].str[2:]\n",
    "adjacent_idc = (ner_dataset['tag'] != 'O') & (ner_dataset['next-tag'] != 'O') & (ner_dataset['entity'] != ner_dataset['next-entity'])\n",
    "sentences_to_remove = ner_dataset[adjacent_idc]['sentence_idx'].values\n",
    "sentences_to_remove\n",
    "\n",
    "ner_dataset=ner_dataset[~ner_dataset['sentence_idx'].isin(sentences_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update tag for discovered metadata values (eg. nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['tag'] = ner_dataset.apply(update_tag_based_on_metadata, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create templates base on NER dataset\n",
    "Here we create the actual templates + handle multiple weird cases that should cause the template sentences to be weird. Note that a manual run over the templates dataset is still required after this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @staticmethod    \n",
    "    def cleanse_template(template, ents):\n",
    "        # Remove whitespace before certain punctuation marks\n",
    "        template = re.sub(r'\\s([?,:.!](?:|$))+', r'\\1', template)\n",
    "        \n",
    "        # Remove whitespaces within double quotes\n",
    "        template = re.sub('\\\"\\s*([^\\\"]*?)\\s*\\\"', r'\"\\1\"', template)    \n",
    "        \n",
    "        # Remove whitespaces within quotes\n",
    "        template = re.sub(\"\\'\\s*([^\\']*?)\\s*\\'\", r\"'\\1'\", template)    \n",
    "        \n",
    "        # Remove whitespaces within parentheses\n",
    "        template = re.sub('\\(\\s*([^\\(]*?)\\s*\\)', r'(\\1)', template)    \n",
    "        \n",
    "        for ent in ents:\n",
    "            #Turn PERSON PERSON into PERSON\n",
    "            duplicates = \"[{}] [{}]\".format(ent,ent)\n",
    "            template = template.replace(duplicates,\"[{}]\".format(ent))\n",
    "        \n",
    "        \n",
    "        # Replace additional weird templates:\n",
    "        to_replace = {\n",
    "            \"[LOCATION] says\" : \"[PERSON] says\",\n",
    "            \"[LOCATION] said\" : \"[PERSON] said\",\n",
    "            \"[ORGANIZATION] of [ORGANIZATION]\" : \"[ORGANIZATION]\",\n",
    "            \"the [COUNTRY]\" : \"[COUNTRY]\",\n",
    "            \" 's \":\"'s\",\n",
    "            \"] 's \":\"]'s \",\n",
    "            \"] 's,\":\"]'s,\",\n",
    "            \"] 's.\":\"]'s.\",\n",
    "            \" n't\" : \"n't\",\n",
    "            \"/?\":\"?\",\n",
    "            \"%u\":\"u\",\n",
    "            \"%m\":\"m\",\n",
    "            \"%e\":\"e\",  \n",
    "            \"%h\":\"h\",  \n",
    "            \"%a\":\"a\",\n",
    "            \" %\":\"%\",\n",
    "            \" ?\":\"?\",\n",
    "            \" /?\":\"?\",\n",
    "            \" ' .\":\"'.\",\n",
    "            \"[ \":\"(\",\n",
    "            \" ]\":\")\",\n",
    "            \"[PERSON] -- [PERSON]\":\"[PERSON]\",\n",
    "            \"[COUNTRY] -- [ORGANIZATION]\":\"[ORGANIZATION]\",\n",
    "            \"Jews\" : \"[NATIONALITY]\",\n",
    "            \"Chinese\" : \"[NATIONALITY]\",\n",
    "            \"Dutch\" : \"[NATIONALITY]\",\n",
    "            \"[LOCATION], [LOCATION]\":\"[LOCATION]\",\n",
    "            \"[LOCATION] [ORGANIZATION]\":\"[ORGANIZATION]\"\n",
    "        }\n",
    "        \n",
    "        for weird in to_replace.keys():\n",
    "            #if weird in template:\n",
    "            #    print(\"Weird sentence\",template)\n",
    "            template = template.replace(weird,to_replace[weird])\n",
    "  \n",
    "        template = template.replace(\" -- \",\" - \")\n",
    "        \n",
    "        #Ignore templates that are incomplete\n",
    "        if \"/-\" in template:\n",
    "            template = \"\"\n",
    "            \n",
    "        #Ignore templates that have numbers after the end or start of the entity\n",
    "        if len(re.findall(r\"\\]\\s[0-9]\",template)) > 0:\n",
    "            template = \"\"\n",
    "            \n",
    "        if len(re.findall(r\"[0-9]\\s\\[\",template)) > 0:\n",
    "            template = \"\"\n",
    "            \n",
    "        if len(re.findall(r\"[0-9].\\s\\[\",template)) > 0:\n",
    "            template = \"\"\n",
    "            \n",
    "            \n",
    "        if \"[PERSON] ([COUNTRY])\" in template:\n",
    "            template = \"\"\n",
    "        if \"[PERSON] ([LOCATION])\" in template:\n",
    "            template = \"\"\n",
    "            \n",
    "        if template.count('\"') == 1:\n",
    "            template = template.replace('\"','')\n",
    "\n",
    "        return template\n",
    "    \n",
    "    @staticmethod    \n",
    "    def get_template(grouped,entity_name_replace_dict):\n",
    "        template = \"\"\n",
    "        i=0\n",
    "        cur_index = 0\n",
    "        ents = []\n",
    "        for token in grouped:\n",
    "            # remove brackets as they interefere with the data generation process\n",
    "            token_text = token[0].replace(\"[\", \"(\").replace(\"]\",\")\")\n",
    "            token_text = token[0].replace(\"{\", \"(\").replace(\"}\",\")\")\n",
    "            token_tag = token[1]\n",
    "            token_entity = token_tag[2:] if len(token_tag)>1 else token_tag\n",
    "            \n",
    "            if token_entity == 'O':\n",
    "                template += \" \" + token_text\n",
    "            elif 'B-' in token_tag and token_entity not in TAGS_TO_IGNORE:\n",
    "                #print(\"found entity: {}\".format(token_entity))\n",
    "                ent = entity_name_replace_dict[token_entity]\n",
    "                ents.append(ent)\n",
    "                 \n",
    "                template += \" [\" + ent + \"]\"\n",
    "            #print(\"template: \",template)\n",
    "        \n",
    "        template = SentenceGetter.cleanse_template(template, ents)\n",
    "        \n",
    "        return template.strip()\n",
    "    \n",
    "getter = SentenceGetter(ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_DICTIONARY = {\"PERSON\":\"PERSON\",\n",
    "                       \"PER\":\"PERSON\",\n",
    "                       \"GPE\":\"COUNTRY\",\n",
    "                       \"NORP\":\"LOCATION\",\n",
    "                       \"LOC\":\"LOCATION\",\n",
    "                       \"ORG\":\"ORGANIZATION\",\n",
    "                       \"MALE_TITLE\":\"MALE_TITLE\",\n",
    "                       \"FEMALE_TITLE\":\"FEMALE_TITLE\",\n",
    "                       \"COUNTRY\":\"COUNTRY\",\n",
    "                       \"NATIONALITY\":\"NATIONALITY\",\n",
    "                       \"NATION_WOMAN\":\"NATION_WOMAN\",\n",
    "                       \"NATION_MAN\":\"NATION_MAN\",\n",
    "                       \"NATION_PLURAL\":\"NATION_PLURAL\"}\n",
    "\n",
    "sentences = getter.sentences\n",
    "\n",
    "sent_id = 445\n",
    "\n",
    "print(\"original:\",sentences[sent_id])\n",
    "print(\"template:\", getter.get_template(sentences[sent_id],entity_name_replace_dict=ENTITIES_DICTIONARY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_templates = [getter.get_template(sentence,entity_name_replace_dict=ENTITIES_DICTIONARY) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original length of templates: {}\".format(len(all_templates)))\n",
    "all_templates = list(set(all_templates))\n",
    "print(\"length after duplicates removal: {}\".format(len(all_templates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save templates to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../raw_data/conll_based_templates.txt\",\"w+\",encoding='utf-8') as f:\n",
    "    for template in all_templates:\n",
    "        f.write(\"%s\\n\" % template)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}